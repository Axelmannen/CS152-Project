{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crR-kxMlG7W2"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "COe6EtcDHTvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Config\n",
        "DATASET_FOLDER_PATH = \"/content/drive/Shareddrives/CS152 Project/dataset/\"\n",
        "DATASET_TREE_PATH = \"/content/drive/Shareddrives/CS152 Project/dataset/all/\""
      ],
      "metadata": {
        "id": "nM0pvvvtHVa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(f\"{DATASET_FOLDER_PATH}train_all_models_fixed.csv\")\n",
        "test_df = pd.read_csv(f\"{DATASET_FOLDER_PATH}test_all_models.csv\")\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "FkAKfkGyHYFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = train_df[[\"dima806_score\", \"wvolf_score\", \"Gemini Classification\"]].values\n",
        "\n",
        "\n",
        "targets = train_df[\"is_ai\"].values\n",
        "\n",
        "# Convert to tensors\n",
        "X_tensor = torch.tensor(features, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(targets.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "# Split into train and validation sets\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# MLP - we tested multiple versions of this\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 4),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Train loop\n",
        "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for val_X, val_y in val_loader:\n",
        "            val_outputs = model(val_X)\n",
        "            v_loss = criterion(val_outputs, val_y)\n",
        "            val_loss += v_loss.item() * val_X.size(0)\n",
        "\n",
        "    train_epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    val_epoch_loss = val_loss / len(val_loader.dataset)\n",
        "\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    val_losses.append(val_epoch_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {train_epoch_loss:.4f}, Val Loss = {val_epoch_loss:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aNdK5YWRIA7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add predictions to test dataframe\n",
        "test_features = test_df[[\"dima806_score\", \"wvolf_score\", \"Gemini Classification\"]].values\n",
        "test_tensor = torch.tensor(test_features, dtype=torch.float32)\n",
        "test_dataset = TensorDataset(test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "test_predictions = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for test_X, in test_loader:\n",
        "        test_outputs = model(test_X)\n",
        "        test_predictions.extend(test_outputs.numpy().flatten())\n",
        "\n",
        "test_df[\"predictions\"] = test_predictions"
      ],
      "metadata": {
        "id": "XLM7iDkDfkAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add majority vote column, between wvolf, dima806 and Gemini Classification\n",
        "majority_vote = []\n",
        "for index, row in test_df.iterrows():\n",
        "  predictions = [row[\"wvolf_score\"], row[\"dima806_score\"], row[\"Gemini Classification\"]]\n",
        "  predictions = [1 if prediction > 0.5 else 0 for prediction in predictions]\n",
        "  if sum(predictions) >= 2:\n",
        "    majority_vote.append(1)\n",
        "  else:\n",
        "    majority_vote.append(0)\n",
        "\n",
        "test_df[\"majority_vote\"] = majority_vote"
      ],
      "metadata": {
        "id": "lX1UJ-2PITfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of zeros and 1s in predictions\n",
        "zeros = 0\n",
        "ones = 0\n",
        "for prediction in test_df[\"majority_vote\"]:\n",
        "  if prediction < 0.5:\n",
        "    zeros += 1\n",
        "  else:\n",
        "    ones += 1\n",
        "print(f\"Zeros: {zeros}\")\n",
        "print(f\"Ones: {ones}\")\n",
        "\n",
        "# Correlation coefficients\n",
        "from scipy.stats import pearsonr\n",
        "print(\"Correlation coefficients:\")\n",
        "print(\"wvolf model:\", pearsonr(test_df[\"wvolf_score\"], test_df[\"is_ai\"]).statistic)\n",
        "print(\"dima806 model:\", pearsonr(test_df[\"dima806_score\"], test_df[\"is_ai\"]).statistic)\n",
        "print(\"gemini model:\", pearsonr(test_df[\"Gemini Classification\"], test_df[\"is_ai\"]).statistic)\n",
        "print(\"majority vote:\", pearsonr(test_df[\"majority_vote\"], test_df[\"is_ai\"]).statistic)\n",
        "# print(\"MLP model:\", pearsonr(test_df[\"predictions\"], test_df[\"is_ai\"]).statistic)"
      ],
      "metadata": {
        "id": "9HgrNc8Of0BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# THRESHOLD = 0.009  # BEST FOR DIMA806\n",
        "# THRESHOLD = 0.08  # Best for wvolf\n",
        "\n",
        "\n",
        "def eval_model(df, column_name, THRESHOLD):\n",
        "  # Calculate eval metrics\n",
        "  true_positives = 0\n",
        "  false_positives = 0\n",
        "  true_negatives = 0\n",
        "  false_negatives = 0\n",
        "  for index, row in df.iterrows():\n",
        "    if row[\"is_ai\"] == 1 and row[column_name] > THRESHOLD:\n",
        "      true_positives += 1\n",
        "    elif row[\"is_ai\"] == 1 and row[column_name] <= THRESHOLD:\n",
        "      false_negatives += 1\n",
        "    elif row[\"is_ai\"] == 0 and row[column_name] > THRESHOLD:\n",
        "      false_positives += 1\n",
        "    elif row[\"is_ai\"] == 0 and row[column_name] <= THRESHOLD:\n",
        "      true_negatives += 1\n",
        "\n",
        "  # assert true_positives + false_positives + true_negatives + false_negatives == len(df)\n",
        "\n",
        "  print(f\"Model: {column_name}\")\n",
        "  print(f\"Dataset size {len(df)}\")\n",
        "  print()\n",
        "  print(f\"True Positives: {true_positives}\")\n",
        "  print(f\"False Positives: {false_positives}\")\n",
        "  print(f\"True Negatives: {true_negatives}\")\n",
        "  print(f\"False Negatives: {false_negatives}\")\n",
        "  print()\n",
        "\n",
        "  accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
        "\n",
        "  if true_positives + false_positives == 0:\n",
        "    precision = 0\n",
        "  else:\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives)\n",
        "  if true_positives + false_negatives == 0:\n",
        "    recall = 0\n",
        "  else:\n",
        "    recall = true_positives / (true_positives + false_negatives)\n",
        "\n",
        "  if precision + recall == 0:\n",
        "    f1_score = 0\n",
        "  else:\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "  print(f\"Accuracy: {accuracy}\")\n",
        "  print(f\"Precision: {precision}\")\n",
        "  print(f\"Recall: {recall}\")\n",
        "  print(f\"F1 Score: {f1_score}\")\n",
        "  print()\n",
        "  print()"
      ],
      "metadata": {
        "id": "Af4AotL5caIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model(test_df, \"predictions\", 0.3)\n",
        "eval_model(test_df, \"Gemini Classification\", 0.2)\n",
        "test_df"
      ],
      "metadata": {
        "id": "eq83fnpPc51U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}